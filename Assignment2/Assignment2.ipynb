{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This assignment may be worked individually or in pairs. Enter your name/s here:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Kayla Han"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Assignment 2: Decision Trees\n",
    "\n",
    "In this assignment we'll implement the Decision Tree algorithm to classify patients as either having or not having diabetic retinopathy. For this task we'll be using the Diabetic Retinopathy data set, which contains features from the Messidor image set to predict whether an image contains signs of diabetic retinopathy or not. This dataset has `1150` records and `20` attributes (some categorical, some continuous). You can find additional details about the dataset [here](http://archive.ics.uci.edu/ml/datasets/Diabetic+Retinopathy+Debrecen+Data+Set)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attribute Information:\n",
    "\n",
    "0) The binary result of quality assessment. 0 = bad quality 1 = sufficient quality.\n",
    "\n",
    "1) The binary result of pre-screening, where 1 indicates severe retinal abnormality and 0 its lack. \n",
    "\n",
    "2-7) The results of MA detection. Each feature value stand for the number of MAs found at the confidence levels alpha = 0.5, . . . , 1, respectively. \n",
    "\n",
    "8-15) contain the same information as 2-7) for exudates. However, as exudates are represented by a set of points rather than the number of pixels constructing the lesions, these features are normalized by dividing the \n",
    "number of lesions with the diameter of the ROI to compensate different image sizes. \n",
    "\n",
    "16) The euclidean distance of the center of the macula and the center of the optic disc to provide important information regarding the patient's condition. This feature is also normalized with the diameter of the ROI.\n",
    "\n",
    "17) The diameter of the optic disc. \n",
    "\n",
    "18) The binary result of the AM/FM-based classification.\n",
    "\n",
    "19) Class label. 1 = contains signs of Diabetic Retinopathy, 0 = no signs of Diabetic Retinopathy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementation: \n",
    "The function prototypes are given to you, please don't change those. You can add additional helper functions if needed. \n",
    "\n",
    "*Suggestion:* The dataset is substantially big, for the purpose of easy debugging, work with a subset of the data and test your decision tree implementation on that.\n",
    "\n",
    "#### Notes:\n",
    "Parts of this assignment will be **autograded** so a couple of caveats :-\n",
    "- Entropy is calculated using log with base 2, `math.log2(x)`.\n",
    "- For continuous features ensure that the threshold value lies exactly between 2 values. For example, if for feature 2 the best split occurs between 10 and 15 then the threshold value will be set as 12.5. For binary features [0/1] the threshold value will be 0.5.\n",
    "- All values < `thresh_val` go to the left child and all values >= `thresh_val` go to the right child."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Headers\n",
    "# You are welcome to add additional headers if you wish\n",
    "# EXCEPT for scikit-learn... You may NOT use scikit-learn for this assignment!\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from math import log2\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataPoint:\n",
    "    def __str__(self):\n",
    "        return \"< \" + str(self.label) + \": \" + str(self.features) + \" >\"\n",
    "    def __init__(self, label, features):\n",
    "        self.label = label # the classification label of this data point\n",
    "        self.features = features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1. Read data from a CSV file. Put it into a list of `DataPoints`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(filename):\n",
    "    data = []\n",
    "#     your code goes here\n",
    "    file = open(filename, 'r')\n",
    "    lines = file.readlines()\n",
    "    count = 0\n",
    "    for line in lines:\n",
    "        line = [float(val) for val in line.strip().split(\",\")]\n",
    "        label = line[19]\n",
    "        del line[19]\n",
    "        data.append(DataPoint(label, line))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TreeNode:\n",
    "    is_leaf = True          # boolean variable to check if the node is a leaf\n",
    "    feature_idx = None      # index that identifies the feature\n",
    "    thresh_val = None       # threshold value that splits the node\n",
    "    prediction = None       # prediction class (only valid for leaf nodes)\n",
    "    left_child = None       # left TreeNode (all values < thresh_val)\n",
    "    right_child = None      # right TreeNode (all values >= thresh_val)\n",
    "    \n",
    "    def printTree(self, level=0):    # for debugging purposes\n",
    "        print(self)\n",
    "        if self.is_leaf:\n",
    "            print ('-'*level + 'Leaf Node:      predicts ' + str(self.prediction))\n",
    "        else:\n",
    "            print ('-'*level + 'Internal Node:  splits on feature ' \n",
    "                   + str(self.feature_idx) + ' with threshold ' + str(self.thresh_val))\n",
    "            self.left_child.printTree(level+1)\n",
    "            self.right_child.printTree(level+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2. Implement the function `make_prediction` that takes the decision tree root and a `DataPoint` instance and returns the prediction label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prediction(tree_root, data_point):\n",
    "#     your code goes here\n",
    "    cur_node = tree_root\n",
    "    while not cur_node.is_leaf:\n",
    "        if data_point.features[cur_node.feature_idx] < cur_node.thresh_val:\n",
    "            cur_node = cur_node.left_child\n",
    "        else:\n",
    "            cur_node = cur_node.right_child\n",
    "    return cur_node.prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3. Implement the function `split_dataset` given an input data set, a `feature_idx` and the `threshold` for the feature. `left_split` will have all values < `threshold` and `right_split` will have all values >= `threshold`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(data, feature_idx, threshold):\n",
    "    left_split = []\n",
    "    right_split = []\n",
    "#     your code goes here\n",
    "    for line in data:\n",
    "        if line.features[feature_idx] < threshold:\n",
    "            left_split.append(line)\n",
    "        else:\n",
    "            right_split.append(line)\n",
    "    return (left_split, right_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4. Implement the function `calc_entropy` to return the entropy of the input dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_entropy(data):\n",
    "    entropy = 0.0\n",
    "#     your code goes here\n",
    "    total_points = len(data)\n",
    "    yes = 0\n",
    "    no = 0\n",
    "    for line in data:\n",
    "        #print(\"A: \", line.label)\n",
    "        if line.label > 0.0:\n",
    "            yes += 1\n",
    "        else:\n",
    "            no += 1\n",
    "    yes_entropy = 0.0\n",
    "    no_entropy = 0.0\n",
    "    yes_frac = yes/total_points\n",
    "    no_frac = no/total_points\n",
    "    if yes_frac != 0.0 and no_frac != 0.0:\n",
    "        entropy = -(yes_frac)*log2(yes_frac) - (no_frac)*log2(no_frac)\n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q5. Implement the function `calc_best_threshold` which returns the best information gain and the corresponding threshold value for one feature at `feature_idx`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_best_threshold(data, feature_idx):\n",
    "    best_info_gain = 0.0\n",
    "    best_thresh = None\n",
    "#     your code goes here\n",
    "    parent_entropy = calc_entropy(data)\n",
    "\n",
    "    cur_thresh = 0\n",
    "    data.sort(key=lambda point: point.features[feature_idx])\n",
    "    pos = 1\n",
    "    cur_line = data[0]\n",
    "    while pos < len(data):\n",
    "        #print(data[pos].features[feature_idx])\n",
    "        cur_thresh = (data[pos].features[feature_idx] + cur_line.features[feature_idx])/2\n",
    "       # print(cur_thresh)\n",
    "        if (cur_thresh != data[pos].features[feature_idx]):\n",
    "            split_data_left, split_data_right = split_dataset(data, feature_idx, cur_thresh)\n",
    "            left_frac = len(split_data_left)/len(data)\n",
    "            right_frac = len(split_data_right)/len(data)\n",
    "            gain = parent_entropy - ((left_frac * calc_entropy(split_data_left)) + (right_frac * calc_entropy(split_data_right)))\n",
    "          #  print(\"meow: \", gain)\n",
    "            if (gain > best_info_gain):\n",
    "                best_thresh = cur_thresh\n",
    "                best_info_gain = gain\n",
    "        cur_line = data[pos]\n",
    "        pos += 1\n",
    "        \n",
    "    if best_thresh == None:\n",
    "        return (None, None)\n",
    "    \n",
    "    else:\n",
    "        return (best_info_gain, best_thresh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q6. Implement the function `identify_best_split` which returns the best feature to split on for an input dataset, and also returns the corresponding threshold value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_best_split(data):\n",
    "    if len(data) < 2:\n",
    "        return (None, None)\n",
    "    best_feature = None\n",
    "    best_thresh = None\n",
    "#     your code goes here\n",
    "    best_info_gain = None\n",
    "    cur_info_gain = None\n",
    "    cur_thresh = None\n",
    "        \n",
    "    feature_num = len(data[0].features)\n",
    "    for feature in range(feature_num):\n",
    "        cur_info_gain, cur_thresh = calc_best_threshold(data, feature)\n",
    "        if cur_info_gain == None and cur_thresh == None:\n",
    "            return (None, None)\n",
    "        if best_info_gain == None or cur_info_gain > best_info_gain:\n",
    "            best_thresh = cur_thresh\n",
    "            best_info_gain = cur_info_gain\n",
    "            best_feature = feature\n",
    "    return (best_feature, best_thresh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q7. Implement the function `create_leaf_node` which returns a `TreeNode` with `is_leaf=True` and `prediction` set to whichever classification occurs most in the dataset at this node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_leaf_node(data):\n",
    "#     your code goes here\n",
    "    new_node = TreeNode()\n",
    "    new_node.is_leaf = True\n",
    "    labels = [d.label for d in data]\n",
    "    counts = Counter(labels)\n",
    "    new_node.prediction = max(counts)\n",
    "    return new_node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q8. Implement the `create_decision_tree` function. `max_levels` denotes the maximum height of the tree (for example if `max_levels = 1` then the decision tree will only contain the leaf node at the root. [Hint: this is where the recursion happens.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_decision_tree(data, max_levels):\n",
    "#     your code goes here\n",
    "    root_node = addHelper(data, max_levels)\n",
    "    return root_node\n",
    "        \n",
    "def addHelper(data, max_levels):\n",
    "   # print(\"best split: \", identify_best_split(data))\n",
    "    print(\"Before if\")\n",
    "    if (max_levels == 1 or identify_best_split(data) == (None, None)):\n",
    "        new_node = create_leaf_node(data)\n",
    "        return new_node\n",
    "    else:\n",
    "        print(\"after if\")\n",
    "        # find best split\n",
    "        # partition records into smaller subsets\n",
    "        part_feature, part_thresh = identify_best_split(data)\n",
    "       # print(\"Splitting on feature: \" , part_feature)\n",
    "       # print(\"Splitting with threshold: \", part_thresh)\n",
    "        #cur_node.feature_idx = part_feature\n",
    "        new_node = TreeNode()\n",
    "        new_node.is_leaf = False\n",
    "        new_node.feature_idx = part_feature\n",
    "        new_node.thresh_val = part_thresh\n",
    "       # cur_node.thresh_val = part_thresh\n",
    "        part_data_left, part_data_right = split_dataset(data, part_feature, part_thresh)\n",
    "        # child node created for each partition of the data, apply algo to each child\n",
    "        print(\"before recursion\")\n",
    "        new_node.left_child = addHelper(part_data_left, max_levels - 1)\n",
    "        new_node.right_child = addHelper(part_data_right, max_levels - 1)\n",
    "        print(\"after recursion\")\n",
    "        print(new_node)\n",
    "        return new_node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q9. Given a test set, the function `calc_accuracy` returns the accuracy of the classifier. You'll use the `make_prediction` function for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_accuracy(tree_root, data):\n",
    "#     your code goes here\n",
    "    accuracy = 0.0\n",
    "    correct = 0\n",
    "    total = len(data)\n",
    "    for line in data:\n",
    "        pred = make_prediction(tree_root, line)\n",
    "        if pred == line.label:\n",
    "            correct += 1\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q10. Keeping the `max_levels` parameter as 10, use 5-fold cross validation to measure the accuracy of the model. Print the accuracy of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 1150\n",
      "Test set size    : 230\n",
      "Before if\n",
      "after if\n",
      "before recursion\n",
      "Before if\n",
      "after if\n",
      "before recursion\n",
      "Before if\n",
      "after if\n",
      "before recursion\n",
      "Before if\n",
      "after if\n",
      "before recursion\n",
      "Before if\n",
      "after if\n",
      "before recursion\n",
      "Before if\n",
      "Before if\n",
      "after if\n",
      "before recursion\n",
      "Before if\n",
      "after if\n",
      "before recursion\n",
      "Before if\n",
      "after if\n",
      "before recursion\n",
      "Before if\n",
      "after if\n",
      "before recursion\n",
      "Before if\n",
      "Before if\n",
      "after recursion\n",
      "<__main__.TreeNode object at 0x7fc4a01a3550>\n",
      "Before if\n",
      "after recursion\n",
      "<__main__.TreeNode object at 0x7fc4a01a3250>\n",
      "Before if\n",
      "after recursion\n",
      "<__main__.TreeNode object at 0x7fc4a01a33a0>\n",
      "Before if\n",
      "after recursion\n",
      "<__main__.TreeNode object at 0x7fc4a01a3370>\n",
      "after recursion\n",
      "<__main__.TreeNode object at 0x7fc4a01a32e0>\n",
      "Before if\n",
      "after recursion\n",
      "<__main__.TreeNode object at 0x7fc4a01a3310>\n",
      "Before if\n",
      "after recursion\n",
      "<__main__.TreeNode object at 0x7fc4a01a32b0>\n",
      "Before if\n",
      "after recursion\n",
      "<__main__.TreeNode object at 0x7fc4a01a3070>\n",
      "Before if\n",
      "after recursion\n",
      "<__main__.TreeNode object at 0x7fc4a01a3280>\n",
      "Time taken: 9.618091106414795\n",
      "The accuracy on the test set is  50.43478260869565\n",
      "<__main__.TreeNode object at 0x7fc4a01a3280>\n",
      "Internal Node:  splits on feature 14 with threshold 0.0203705\n",
      "<__main__.TreeNode object at 0x7fc4a01a3070>\n",
      "-Internal Node:  splits on feature 2 with threshold 55.5\n",
      "<__main__.TreeNode object at 0x7fc4a01a32b0>\n",
      "--Internal Node:  splits on feature 8 with threshold 127.805703\n",
      "<__main__.TreeNode object at 0x7fc4a01a3310>\n",
      "---Internal Node:  splits on feature 11 with threshold 0.040542499999999995\n",
      "<__main__.TreeNode object at 0x7fc4a01a32e0>\n",
      "----Internal Node:  splits on feature 17 with threshold 0.0866005\n",
      "<__main__.TreeNode object at 0x7fc4a01a33d0>\n",
      "-----Leaf Node:      predicts 1.0\n",
      "<__main__.TreeNode object at 0x7fc4a01a3370>\n",
      "-----Internal Node:  splits on feature 12 with threshold 0.0053685\n",
      "<__main__.TreeNode object at 0x7fc4a01a33a0>\n",
      "------Internal Node:  splits on feature 16 with threshold 0.566381\n",
      "<__main__.TreeNode object at 0x7fc4a01a3250>\n",
      "-------Internal Node:  splits on feature 9 with threshold 21.610879\n",
      "<__main__.TreeNode object at 0x7fc4a01a3550>\n",
      "--------Internal Node:  splits on feature 2 with threshold 3.5\n",
      "<__main__.TreeNode object at 0x7fc4a01a3460>\n",
      "---------Leaf Node:      predicts 1.0\n",
      "<__main__.TreeNode object at 0x7fc4a01a3430>\n",
      "---------Leaf Node:      predicts 1.0\n",
      "<__main__.TreeNode object at 0x7fc4a01a3340>\n",
      "--------Leaf Node:      predicts 1.0\n",
      "<__main__.TreeNode object at 0x7fc4a01a3580>\n",
      "-------Leaf Node:      predicts 0.0\n",
      "<__main__.TreeNode object at 0x7fc4a01a3490>\n",
      "------Leaf Node:      predicts 1.0\n",
      "<__main__.TreeNode object at 0x7fc4a01a34c0>\n",
      "----Leaf Node:      predicts 1.0\n",
      "<__main__.TreeNode object at 0x7fc4a01a34f0>\n",
      "---Leaf Node:      predicts 1.0\n",
      "<__main__.TreeNode object at 0x7fc4a01a3520>\n",
      "--Leaf Node:      predicts 1.0\n",
      "<__main__.TreeNode object at 0x7fc4a01a31c0>\n",
      "-Leaf Node:      predicts 1.0\n"
     ]
    }
   ],
   "source": [
    "# edit the code here - this is just a sample to get you started\n",
    "import time\n",
    "\n",
    "d = get_data(\"messidor_features.txt\")\n",
    "\n",
    "# partition data into train_set and test_set\n",
    "num_per_partition = len(d) / 5\n",
    "\n",
    "train_set = d[0:1150]\n",
    "test_set = d[920:1150]\n",
    "\n",
    "print ('Training set size:', len(train_set))\n",
    "print ('Test set size    :', len(test_set))\n",
    "\n",
    "# the timer is just for fun! you will NOT be graded on runtime\n",
    "start = time.time()\n",
    "\n",
    "# create the decision tree\n",
    "tree = create_decision_tree(train_set, 10)\n",
    "\n",
    "end = time.time()\n",
    "print ('Time taken:', end - start)\n",
    "\n",
    "# calculate the accuracy of the tree\n",
    "accuracy = calc_accuracy(tree, test_set)\n",
    "print ('The accuracy on the test set is ', str(accuracy * 100.0))\n",
    "tree.printTree()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2127024793.py, line 30)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/var/folders/2l/qnm9n6zn6md64z8ty97r_xbc0000gn/T/ipykernel_3856/2127024793.py\"\u001b[0;36m, line \u001b[0;32m30\u001b[0m\n\u001b[0;31m    run code snippetVisit Manage Class to disable runnable code snippets×\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SMALL DATASET\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'sample_train.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/qg/tmx1_k7x1l32yhxk9gb5p0m80000gn/T/ipykernel_24714/4096181050.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m## SMALL DATASET\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\nSMALL DATASET'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtemp_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"sample_train.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Length of dataset: {len(temp_data)}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/qg/tmx1_k7x1l32yhxk9gb5p0m80000gn/T/ipykernel_24714/4085242575.py\u001b[0m in \u001b[0;36mget_data\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#     your code goes here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'sample_train.txt'"
     ]
    }
   ],
   "source": [
    "#TEST CODE\n",
    "\n",
    "## SMALL DATASET\n",
    "print('\\nSMALL DATASET')\n",
    "temp_data = get_data(\"sample_train.txt\")\n",
    "print(f'Length of dataset: {len(temp_data)}')\n",
    "\n",
    "print(f'Entropy of dataset: {calc_entropy(temp_data)}')\n",
    "\n",
    "best_gain, best_thresh = calc_best_threshold(temp_data, 3)\n",
    "print(\"Best thresh:\", best_thresh)\n",
    "print(\"Best gain:\", best_gain)\n",
    "\n",
    "feat, thresh = identify_best_split(temp_data)\n",
    "print(f\"Best split: feature_index={feat}, thresh={thresh}\")\n",
    "\n",
    "## FULL DATASET\n",
    "print('\\nFULL DATASET')\n",
    "full_data = get_data(\"messidor_features.txt\")\n",
    "print(f'Length of dataset: {len(full_data)}')\n",
    "\n",
    "print(f'Entropy of dataset: {calc_entropy(full_data)}')\n",
    "\n",
    "best_gain, best_thresh = calc_best_threshold(full_data, 3)\n",
    "print(\"Best thresh:\", best_thresh)\n",
    "print(\"Best gain:\", best_gain)\n",
    "\n",
    "feat, thresh = identify_best_split(full_data)\n",
    "print(f\"Best split: feature_index={feat}, thresh={thresh}\")\n",
    "\n",
    "print(\"Create Tree:\")\n",
    "temp_tree = create_decision_tree(temp_data, 10)\n",
    "temp_tree.printTree()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run code snippetVisit Manage Class to disable runnable code snippets×\n",
    "These are the answers you should get:\n",
    "\n",
    "SMALL DATASET\n",
    "Length of dataset: 15\n",
    "Entropy of dataset: 0.9709505944546686\n",
    "Best thresh: 35.0\n",
    "Best gain: 0.18580516288960103\n",
    "Best split: feature_index=6, thresh=25.5\n",
    "\n",
    "FULL DATASET\n",
    "Length of dataset: 1150\n",
    "Entropy of dataset: 0.9971705766292643\n",
    "Best thresh: 62.5\n",
    "Best gain: 0.04595772209113502\n",
    "Best split: feature_index=14, thresh=0.0203705"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
